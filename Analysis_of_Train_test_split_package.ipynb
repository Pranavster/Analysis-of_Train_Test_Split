{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior probability of positive class in the training set: 0.5576702557000104\n",
      "Prior probability of negative class in the training set: 0.4423297442999896\n"
     ]
    }
   ],
   "source": [
    "df_sst = pd.read_csv(\"SST-2/train.tsv\", delimiter=\"\\t\")\n",
    "labels = df_sst['label']\n",
    "X_train, X_temp, y_train, y_temp=train_test_split(df_sst, labels, test_size=(100 + 100), random_state=42)\n",
    "X_validation, X_test, y_validation, y_test=train_test_split(X_temp, y_temp, test_size=100, random_state=42)\n",
    "prior_prob_positive=y_train.sum()/len(y_train)\n",
    "prior_prob_negative=1-prior_prob_positive\n",
    "print(\"Prior probability of positive class in the training set:\", prior_prob_positive)\n",
    "print(\"Prior probability of negative class in the training set:\", prior_prob_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization of the first sentence in the training set:\n",
      "<class 'list'>\n",
      "\n",
      "Vocabulary size of the training set (including start and end symbols): 14817\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(sentence):\n",
    "    retList=['<s>']\n",
    "    tokenized_sentence=sentence.split()\n",
    "    for word in tokenized_sentence:\n",
    "        retList.append(word)\n",
    "    retList.append('</s>')\n",
    "    return retList\n",
    "\n",
    "X_train['tokenized_sentence']=X_train['sentence'].apply(tokenizer)\n",
    "print(\"Tokenization of the first sentence in the training set:\")\n",
    "print(type(X_train['tokenized_sentence'].iloc[0]))\n",
    "vocabulary=set()\n",
    "for tokens in X_train['tokenized_sentence']:\n",
    "    vocabulary.update(tokens)\n",
    "\n",
    "vocabulary_size=len(vocabulary)\n",
    "print(\"\\nVocabulary size of the training set (including start and end symbols):\", vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bigram ('<s>', 'the') occurs 4451 times in the training set:\n"
     ]
    }
   ],
   "source": [
    "def bigram_counts_organizer(tokenized_sequences):\n",
    "    bigram_counts={}\n",
    "    for sequence in tokenized_sequences:\n",
    "        for i in range(len(sequence)-1):\n",
    "            wi=sequence[i]\n",
    "            wj=sequence[i+1]\n",
    "            if wi not in bigram_counts:\n",
    "                bigram_counts[wi] ={}\n",
    "            if wj not in bigram_counts[wi]:\n",
    "                bigram_counts[wi][wj]=0\n",
    "            bigram_counts[wi][wj]+=1\n",
    "    return bigram_counts\n",
    "\n",
    "bigram_counts = bigram_counts_organizer(X_train['tokenized_sentence'])\n",
    "start_with_the_count = bigram_counts[\"<s>\"].get(\"the\", 0)\n",
    "\n",
    "print(\"The bigram ('<s>', 'the') occurs\", start_with_the_count,\"times in the training set:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0251860898691059"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Lidstone_smoothing(curr_word,prev_word,bigram_counts,alpha,vocab_size):\n",
    "    count_bigram=0\n",
    "    if prev_word in bigram_counts and curr_word in bigram_counts[prev_word]:\n",
    "        count_bigram=bigram_counts[prev_word][curr_word]\n",
    "    count_prev_word=sum(bigram_counts.get(prev_word, {}).values())\n",
    "    return math.log(((count_bigram+alpha)/(count_prev_word+alpha*vocab_size)))\n",
    "\n",
    "Lidstone_smoothing(\"award\",\"academy\",bigram_counts,.001,vocabulary_size)#these words are solely based on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.173181082203538"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lidstone_smoothing(\"award\",\"academy\",bigram_counts,.5,vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-80.27771915479497"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def log_prob_calculator(sentence,bigram_counts,alpha,vocabulary_size):\n",
    "    sentence=sentence.split()\n",
    "    log_probability=0\n",
    "    prev_word=\"<s>\"\n",
    "    for curr_word in sentence:\n",
    "        log_probability+=Lidstone_smoothing(curr_word,prev_word,bigram_counts,alpha,vocabulary_size)\n",
    "        prev_word=curr_word\n",
    "    return log_probability\n",
    "natural_English=\"this was a really great movie but it was a little too long.\"\n",
    "unnatural_English=\"long too little a was it but movie great really a was this.\"\n",
    "\n",
    "log_prob_calculator(natural_English,bigram_counts,.0001,vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-173.9217151851817"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob_calculator(unnatural_English,bigram_counts,.0001,vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5787.20481731709\n"
     ]
    }
   ],
   "source": [
    "X_validation['tokenized_sentence']=X_validation['sentence'].apply(tokenizer)\n",
    "tokenized_sentence_list=X_validation['tokenized_sentence'].tolist()\n",
    "tokenized_sentence_list.remove(tokenized_sentence_list[0])\n",
    "tokenized_sentence_list.remove(tokenized_sentence_list[-1])\n",
    "tokenized_sentence_list=[word for sentence in tokenized_sentence_list for word in sentence]\n",
    "tokenized_sentence_list=tokenized_sentence_list[1:-1]\n",
    "tokenized_sentence=' '.join(tokenized_sentence_list)\n",
    "selected_alpha=log_prob_calculator(tokenized_sentence,bigram_counts,.001,vocabulary_size)\n",
    "print(selected_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6322.267898978088"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob_calculator(tokenized_sentence,bigram_counts,.01,vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7415.913096828978"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob_calculator(tokenized_sentence,bigram_counts,.1,vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of Predicted Sentiments: {'Predicted Positive Sentiment': 45, 'Predicted Negative Sentiment': 55}\n",
      "Accuracy: 0.52\n"
     ]
    }
   ],
   "source": [
    "positive_sentences=X_train[X_train['label'] == 1]['tokenized_sentence'].tolist()\n",
    "negative_sentences=X_train[X_train['label'] == 0]['tokenized_sentence'].tolist()\n",
    "positive_vocab_size=len(set(word for sentence in positive_sentences for word in sentence))\n",
    "negative_vocab_size=len(set(word for sentence in negative_sentences for word in sentence))\n",
    "positive_bigram_counts=bigram_counts_organizer(positive_sentences)\n",
    "negative_bigram_counts=bigram_counts_organizer(negative_sentences)\n",
    "predicted_labels=[]\n",
    "true_labels=X_test['label'].tolist()\n",
    "correct_predictions=0\n",
    "for i, sentence in enumerate(X_test['sentence']):\n",
    "    log_odds_positive=log_prob_calculator(sentence,positive_bigram_counts,selected_alpha,positive_vocab_size)+math.log(prior_prob_positive)-log_prob_calculator(sentence,negative_bigram_counts,selected_alpha,negative_vocab_size)-math.log(prior_prob_negative)\n",
    "    if log_odds_positive<0:\n",
    "        predicted_label=0\n",
    "    else:\n",
    "        predicted_label=1\n",
    "    predicted_labels.append(predicted_label)\n",
    "    if predicted_label==true_labels[i]:\n",
    "        correct_predictions += 1\n",
    "class_distribution={\n",
    "    \"Predicted Positive Sentiment\":predicted_labels.count(1),\n",
    "    \"Predicted Negative Sentiment\":predicted_labels.count(0)\n",
    "}\n",
    "print(\"Distribution of Predicted Sentiments:\",class_distribution)\n",
    "print(\"Accuracy:\",correct_predictions/len(X_test))#coin flip, 1/2 ull get the right answer,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
